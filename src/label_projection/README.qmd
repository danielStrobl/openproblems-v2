---
format: gfm
info:
  migration_date: "2022-10-17 12:49:00 GMT"
---

```{r setup, include=FALSE}
library(tidyverse)
library(rlang)

strip_margin <- function(text, symbol = "\\|") {
  str_replace_all(text, paste0("(\n?)[ \t]*", symbol), "\\1") 
}

dir <- "src/label_projection"
dir <- "."
```

# Label Projection

## The task

A major challenge for integrating single cell datasets is creating matching cell type annotations for each cell. One of the most common strategies for annotating cell types is referred to as ["cluster-then-annotate"](https://www.nature.com/articles/s41576-018-0088-9) whereby cells are aggregated into clusters based on feature similarity and then manually characterized based on differential gene expression or previously identified marker genes. Recently, methods have emerged to build on this strategy and annotate cells using [known marker genes](https://www.nature.com/articles/s41592-019-0535-3). However, these strategies pose a difficulty for integrating atlas-scale datasets as the particular annotations may not match.

To ensure that the cell type labels in newly generated datasets match existing reference datasets, some methods align cells to a previously annotated [reference dataset](https://academic.oup.com/bioinformatics/article/35/22/4688/54802990) and then _project_ labels from the reference to the new dataset.

Here, we compare methods for annotation based on a reference dataset. The datasets consist of two or more samples of single cell profiles that have been manually annotated with matching labels. These datasets are then split into training and test batches, and the task of each method is to train a cell type classifer on the training set and project those labels onto the test set.

## The metrics

Metrics for label projection aim to characterize how well each classifer correctly assigns cell type labels to cells in the test set.

* **Accuracy**: Average number of correctly applied labels.
* **F1 score**: The [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) is a weighted average of the precision and recall over all class labels, where an F1 score reaches its best value at 1 and worst score at 0, where each class contributes to the score relative to its frequency in the dataset.
* **Macro F1 score**: The macro F1 score is an unweighted F1 score, where each class contributes equally, regardless of its frequency.

## API

```{r data, include=FALSE}
yaml_files <- list.files(paste0(dir, "/api"), full.names = TRUE)

file_arg_info <- map_df(yaml_files, function(yaml_file) {
  conf <- yaml::read_yaml(yaml_file)

  map_df(conf$functionality$arguments, function(arg) {
    tibble(
      comp = conf$functionality$name,
      arg_name = str_replace_all(arg$name, "^-*", ""),
      id = paste0(comp, "__", arg_name),
      short_description = arg$meta$short_description,
      description = arg$description,
      direction = arg$direction %||% "input",
      from = arg$meta$from
    )
  })
})

comp_info <- map_df(yaml_files, function(yaml_file) {
  conf <- yaml::read_yaml(yaml_file)

  tibble(
    name = conf$functionality$name,
    label = conf$functionality$meta$label %||% str_replace_all(name, "_", " ")
  )
})

slot_info <- map_df(yaml_files, function(yaml_file) {
  conf <- yaml::read_yaml(yaml_file)

  map_df(conf$functionality$arguments, function(arg) {
    out <- map2_df(names(arg$slots), arg$slots, function(group_name, slot) {
      df <- map_df(slot, as.data.frame)
      df$struct <- group_name
      as_tibble(df)
    })
    out$component <- conf$functionality$name
    out$argument <- str_replace_all(arg$name, "^-*", "")
    out$direction <- arg$direction %||% "input"
    out
  })
}) %>% 
  select(component, argument, struct, name, everything()) %>%
  mutate(multiple = multiple %|% FALSE, id = paste0(component, "__", argument))
```

```{r flow, echo=FALSE,warning=FALSE,error=FALSE}
nodes <- bind_rows(
  file_arg_info %>%
    filter(!is.na(short_description)) %>%
    transmute(id, label = short_description, is_comp = FALSE),
  comp_info %>%
    transmute(id = name, label, is_comp = TRUE)
) %>%
  mutate(str = paste0(
    "  ",
    id, 
    ifelse(is_comp, "[/", "("), 
    label,
    ifelse(is_comp, "/]", ")")
  ))
edges <- bind_rows(
  file_arg_info %>%
    filter(direction == "input") %>%
    transmute(from = ifelse(!is.na(from), from, id), to = comp, arrow = "---"),
  file_arg_info %>%
    filter(direction == "output") %>%
    transmute(from = comp, to = id, arrow = "-->")
) %>%
  mutate(str = paste0("  ", from, arrow, to))

# note: use ```{mermaid} instead of ```mermaid when rendering to html
out_str <- strip_margin(glue::glue("
  §```mermaid
  §%%| column: screen-inset-shaded
  §flowchart LR
  §{paste(nodes$str, collapse = '\n')}
  §{paste(edges$str, collapse = '\n')}
  §```
  §"), symbol = "§")
knitr::asis_output(out_str)
```

```{r api, echo=FALSE,warning=FALSE,error=FALSE,output="asis"}
obj_ids <- file_arg_info %>% filter(!is.na(short_description)) %>% pull(id)
for (obj_id in obj_ids) {
  arg_info <- file_arg_info %>% filter(id == obj_id)
  sub_out <- slot_info %>% 
    filter(id == obj_id) %>% 
    select(struct, name, type, description)
  used_in <- file_arg_info %>%
    filter(id == obj_id | from == obj_id) %>%
    mutate(str = paste0("* ", comp, ": ", arg_name, " (as ", direction, ")")) %>%
    pull(str)
  
  out_str <- strip_margin(glue::glue("
    §### {arg_info$short_description}
    §
    §{arg_info$description}
    §
    §Used in:
    §
    §{paste(used_in, collapse = '\n')}
    §
    §Slots:
    §
    §{paste(knitr::kable(sub_out, format = 'pipe'), collapse = '\n')}
    §"), symbol = "§")
  cat(out_str)
}
```

<!--
Datasets should contain the following attributes:

* `adata.obs["labels"]` with ground truth celltype labels,
* `adata.obs["batch"]` with information of batches in the data, and
* `adata.obs["is_train"]` with a train vs. test split

It should be noted that datasets may only contain a single batch, or not contain discriminative batch information.

Methods should assign output celltype labels to `adata.obs['labels_pred']` using only the labels from the training data.

Note that the true labels are contained in `adata['labels']`.

Metrics can compare `adata['labels']` to `adata.obs['labels_pred']` using only the labels from the test data.
-->