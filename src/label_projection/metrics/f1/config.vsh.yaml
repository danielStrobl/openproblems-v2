__inherits__: ../../api/comp_metric.yaml
functionality:
  name: "f1"
  namespace: "label_projection/metrics"
  description: "balanced F-score or F-measure"
  arguments:
    - name: "--average"
      type: "string"
      default: "weighted"
      choices: ['micro', 'macro', 'samples', 'weighted']
      description: |
        Determines the type of averaging performed on the data.

          - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.
          - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.
          - 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.
          - 'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).
  resources:
    - type: python_script
      path: script.py
platforms:
  - type: docker
    image: "python:3.10"
    setup:
      - type: python
        packages:
          - scikit-learn
          - "anndata<0.8"
  - type: nextflow
